{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with sklearn - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll learn how to use sklearn's implementation of a KNN classifier  on some real world datasets!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Use KNN to make classification predictions on a real-world dataset\n",
    "* Perform a parameter search for 'k' to optimize model performance\n",
    "* Evaluate model performance and interpret results\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this lab, we'll make use of sklearn's implementation of the **_K-Nearest Neighbors_** algorithm. We'll use it to make predictions on the Titanic dataset. \n",
    "\n",
    "We'll start by importing the dataset, and then deal with preprocessing steps such as removing unnecessary columns and normalizing our dataset.\n",
    "\n",
    "You'll find the titanic dataset stored in the `titanic.csv` file. In the cell below:\n",
    "\n",
    "* Import pandas and set the standard alias.\n",
    "* Read in the data from `titanic.csv` and store it in a pandas DataFrame. \n",
    "* Print the head of the DataFrame to ensure everything loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, we'll preprocess our data to get it ready for use with a KNN classifier.\n",
    "\n",
    "## Preprocessing Our Data\n",
    "\n",
    "This stage should be pretty familiar to you by now. Although it's not the fun part of machine learning, it's good practice to get used to it.  Although it isn't as fun or exciting as training machine learning algorithms, it's a very large, very important part of the Data Science Process. As a Data Scientist, you'll often spend the majority of your time wrangling and preprocessing, just to get it ready for use with supervised learning algorithms. \n",
    "\n",
    "Since you've done this before, you should be able to do this quite well yourself without much hand holding by now. \n",
    "\n",
    "In the cells below, complete the following steps:\n",
    "\n",
    "1. Remove unnecessary columns (PassengerId, Name, Ticket, and Cabin).\n",
    "2. Convert `Sex` to a binary encoding, where female is `0` and male is `1`.\n",
    "3. Detect and deal with any null values in the dataset. \n",
    "    * For `Age`, replace null values with the median age for the dataset. \n",
    "    * For `Embarked`, drop the rows that contain null values\n",
    "4. One-Hot Encode categorical columns such as `Embarked`.\n",
    "5. Store our target column, `Survived`, in a separate variable and remove it from the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns = ['PassengerId', 'Name', 'Ticket','Cabin'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sex to a binary encoding, where female is 0 and male is 1.\n",
    "X.Sex = X.Sex.map({'female': 0, 'male': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    1  22.0      1      0   7.2500        S\n",
       "1         1       1    0  38.0      1      0  71.2833        C\n",
       "2         1       3    0  26.0      0      0   7.9250        S\n",
       "3         1       1    0  35.0      1      0  53.1000        S\n",
       "4         0       3    1  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Survived    0\n",
       " Pclass      0\n",
       " Sex         0\n",
       " Age         0\n",
       " SibSp       0\n",
       " Parch       0\n",
       " Fare        0\n",
       " Embarked    0\n",
       " dtype: int64, (889, 8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Detect and deal with any null values in the dataset.\n",
    "# For Age, replace null values with the median age for the dataset.\n",
    "# For Embarked, drop the rows that contain null values\n",
    "import numpy as np\n",
    "X.Age = X.Age.fillna(X.Age.median())\n",
    "X.dropna(inplace = True)\n",
    "X.isna().sum(), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final = pd.get_dummies(X)\n",
    "y = X_final.Survived\n",
    "X_final.drop(columns=['Embarked_Q', 'Survived'], inplace = True)\n",
    "y.shape[0] == X_final.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Our Data\n",
    "\n",
    "Good job preprocessing our data! This can seem tedious, but its a very important foundational skill in any Data Science toolbox. The final step we we'll take in our preprocessing efforts is to **_Normalize_** our data. Recall that normalization (also sometimes called **_Standardization_** or **_Scaling_**) means making sure that all of our data is represented at the same scale.  The most common way to do this is to convert all numerical values to z-scores. \n",
    "\n",
    "Since KNN is a distance-based classifier, data on different scales and negatively affect the results of our model! Predictors on much larger scales will overwhelm data with much smaller scales, because euclidean distance is going to treat them as the same.\n",
    "\n",
    "To scale our data, we'll make use of the `StandardScaler` object found inside the `sklearn.preprocessing` module. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import and instantiate a `StandardScaler` object. \n",
    "* Use the scaler's `.fit_transform()` method to create a scaled version of our dataset. \n",
    "* The result returned by the `fit_transform` call will be a numpy array, not a pandas DataFrame. Create a new pandas DataFrame out of this object called `scaled_df`. To set the column names back to their original state, set the `columns` parameter to `one_hot_df.columns`.\n",
    "* Print out the head of `scaled_df` to ensure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C',\n",
       "       'Embarked_S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dont forget to import!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_new = scaler.fit_transform(X_final)\n",
    "X_scaled = pd.DataFrame(X_new, columns=[X_final.columns])\n",
    "X_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass^2</th>\n",
       "      <th>Pclass Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch^2</th>\n",
       "      <th>Parch Fare</th>\n",
       "      <th>Parch Embarked_C</th>\n",
       "      <th>Parch Embarked_S</th>\n",
       "      <th>Fare^2</th>\n",
       "      <th>Fare Embarked_C</th>\n",
       "      <th>Fare Embarked_S</th>\n",
       "      <th>Embarked_C^2</th>\n",
       "      <th>Embarked_C Embarked_S</th>\n",
       "      <th>Embarked_S^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825209</td>\n",
       "      <td>0.735342</td>\n",
       "      <td>-0.563674</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.500240</td>\n",
       "      <td>-0.482711</td>\n",
       "      <td>0.616794</td>\n",
       "      <td>0.680969</td>\n",
       "      <td>0.606811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224985</td>\n",
       "      <td>0.237277</td>\n",
       "      <td>0.228962</td>\n",
       "      <td>-0.292561</td>\n",
       "      <td>0.250240</td>\n",
       "      <td>0.241471</td>\n",
       "      <td>-0.308545</td>\n",
       "      <td>0.233010</td>\n",
       "      <td>-0.297733</td>\n",
       "      <td>0.380435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.572211</td>\n",
       "      <td>-1.359911</td>\n",
       "      <td>0.669217</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>0.788947</td>\n",
       "      <td>2.071634</td>\n",
       "      <td>-1.621287</td>\n",
       "      <td>2.471848</td>\n",
       "      <td>2.138068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224985</td>\n",
       "      <td>-0.374218</td>\n",
       "      <td>-0.982629</td>\n",
       "      <td>0.769018</td>\n",
       "      <td>0.622437</td>\n",
       "      <td>1.634408</td>\n",
       "      <td>-1.279109</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>-3.358713</td>\n",
       "      <td>2.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.825209</td>\n",
       "      <td>-1.359911</td>\n",
       "      <td>-0.255451</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.486650</td>\n",
       "      <td>-0.482711</td>\n",
       "      <td>0.616794</td>\n",
       "      <td>0.680969</td>\n",
       "      <td>-1.122211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224985</td>\n",
       "      <td>0.230831</td>\n",
       "      <td>0.228962</td>\n",
       "      <td>-0.292561</td>\n",
       "      <td>0.236828</td>\n",
       "      <td>0.234911</td>\n",
       "      <td>-0.300163</td>\n",
       "      <td>0.233010</td>\n",
       "      <td>-0.297733</td>\n",
       "      <td>0.380435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.572211</td>\n",
       "      <td>-1.359911</td>\n",
       "      <td>0.438050</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>0.422861</td>\n",
       "      <td>-0.482711</td>\n",
       "      <td>0.616794</td>\n",
       "      <td>2.471848</td>\n",
       "      <td>2.138068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224985</td>\n",
       "      <td>-0.200574</td>\n",
       "      <td>0.228962</td>\n",
       "      <td>-0.292561</td>\n",
       "      <td>0.178812</td>\n",
       "      <td>-0.204120</td>\n",
       "      <td>0.260818</td>\n",
       "      <td>0.233010</td>\n",
       "      <td>-0.297733</td>\n",
       "      <td>0.380435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.825209</td>\n",
       "      <td>0.735342</td>\n",
       "      <td>0.438050</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.484133</td>\n",
       "      <td>-0.482711</td>\n",
       "      <td>0.616794</td>\n",
       "      <td>0.680969</td>\n",
       "      <td>0.606811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224985</td>\n",
       "      <td>0.229637</td>\n",
       "      <td>0.228962</td>\n",
       "      <td>-0.292561</td>\n",
       "      <td>0.234385</td>\n",
       "      <td>0.233696</td>\n",
       "      <td>-0.298610</td>\n",
       "      <td>0.233010</td>\n",
       "      <td>-0.297733</td>\n",
       "      <td>0.380435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Sex       Age     SibSp     Parch      Fare  Embarked_C  \\\n",
       "0  0.825209  0.735342 -0.563674  0.431350 -0.474326 -0.500240   -0.482711   \n",
       "1 -1.572211 -1.359911  0.669217  0.431350 -0.474326  0.788947    2.071634   \n",
       "2  0.825209 -1.359911 -0.255451 -0.475199 -0.474326 -0.486650   -0.482711   \n",
       "3 -1.572211 -1.359911  0.438050  0.431350 -0.474326  0.422861   -0.482711   \n",
       "4  0.825209  0.735342  0.438050 -0.475199 -0.474326 -0.484133   -0.482711   \n",
       "\n",
       "   Embarked_S  Pclass^2  Pclass Sex      ...        Parch^2  Parch Fare  \\\n",
       "0    0.616794  0.680969    0.606811      ...       0.224985    0.237277   \n",
       "1   -1.621287  2.471848    2.138068      ...       0.224985   -0.374218   \n",
       "2    0.616794  0.680969   -1.122211      ...       0.224985    0.230831   \n",
       "3    0.616794  2.471848    2.138068      ...       0.224985   -0.200574   \n",
       "4    0.616794  0.680969    0.606811      ...       0.224985    0.229637   \n",
       "\n",
       "   Parch Embarked_C  Parch Embarked_S    Fare^2  Fare Embarked_C  \\\n",
       "0          0.228962         -0.292561  0.250240         0.241471   \n",
       "1         -0.982629          0.769018  0.622437         1.634408   \n",
       "2          0.228962         -0.292561  0.236828         0.234911   \n",
       "3          0.228962         -0.292561  0.178812        -0.204120   \n",
       "4          0.228962         -0.292561  0.234385         0.233696   \n",
       "\n",
       "   Fare Embarked_S  Embarked_C^2  Embarked_C Embarked_S  Embarked_S^2  \n",
       "0        -0.308545      0.233010              -0.297733      0.380435  \n",
       "1        -1.279109      4.291667              -3.358713      2.628571  \n",
       "2        -0.300163      0.233010              -0.297733      0.380435  \n",
       "3         0.260818      0.233010              -0.297733      0.380435  \n",
       "4        -0.298610      0.233010              -0.297733      0.380435  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing, pipeline\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "features_poly = pd.DataFrame(poly.fit_transform(X_scaled), columns=poly.get_feature_names(X_final.columns))\n",
    "X_poly= features_poly\n",
    "X_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = features_poly.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "plt.title(\"Polynomials Correlation Heatmap\", fontsize = 30, color = \"#F97E77\")\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the scaler also scaled our binary/one-hot encoded columns, too! Although it doesn't look as pretty, this has no negative effect on our model. Each 1 and 0 have been replaced with corresponding decimal values, but each binary column still only contains 2 values, meaning the overall information content of each column has not changed. \n",
    "\n",
    "#### Creating Training and Testing Sets\n",
    "\n",
    "Now that we've preprocessed our data, the only step remaining is to split our data into training and testing sets. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `train_test_split` from the `sklearn.model_selection` module\n",
    "* Use `train_test_split` to split our data into training and testing sets, with a `test_size` of `0.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Fitting our KNN Model\n",
    "\n",
    "Now that we've preprocessed our data successfully, it's time for the fun stuff--let's create a KNN classifier and use it to make predictions on our dataset!  Since you've got some experience on this part from when we built our own model, we won't hold your hand through section. \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import `KNeighborsClassifier` from the `sklearn.neighbors` module.\n",
    "* Instantiate a classifier. For now, we'll just use the default parameters. \n",
    "* Fit the classifier to our training data/labels\n",
    "* Use the classifier to generate predictions on our testing data. Store these predictions inside the variable `test_preds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_hat_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cells below, import all the necessary evaluation metrics from `sklearn.metrics` abd then complete the following `print_metrics()` function so that it prints out **_Precision, Recall, Accuracy,_** and **_F1-Score_** when given a set of `labels` and `preds`. \n",
    "\n",
    "Then, use it to print out the evaluation metrics for our test predictions stored in `test_preds`, and the corresponding labels in `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.7692307692307693\n",
      "Recall Score: 0.7407407407407407\n",
      "Accuracy Score: 0.8251121076233184\n",
      "F1 Score: 0.7547169811320754\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "print_metrics(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.7972972972972973\n",
      "Recall Score: 0.7283950617283951\n",
      "Accuracy Score: 0.8340807174887892\n",
      "F1 Score: 0.7612903225806451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7612903225806451"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polynomial\n",
    "from sklearn.metrics import classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.25, random_state = 18)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_poly = KNeighborsClassifier()\n",
    "knn_poly.fit(X_train, y_train)\n",
    "\n",
    "y_hat_test = knn_poly.predict(X_test)\n",
    "\n",
    "    \n",
    "print_metrics(y_test, y_hat_test)\n",
    "f1_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_QUESTION:_** Interpret each of the metrics above, and explain what they tell us about our model's capabilities. If you had to pick one score to best describe the performance of the model, which would you choose? Explain your answer.\n",
    "\n",
    "Write your answer below this line:\n",
    "________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "## Improving Model Performance\n",
    "\n",
    "Our overall model results are better than random chance, but not by a large margin. For the remainder of this notebook, we'll focus on improving model performance. This is also a big part of the Data Science Process--your first fit is almost never your best. Modeling is an **_iterative process_**, meaning that we should make small incremental changes to our model and use our intuition to see if we can improve the overall performance. \n",
    "\n",
    "First, we'll start off by trying to find the optimal number of neighbors to use for our classifier. To do this, we'll write a quick function that iterates over multiple values of k and finds the one that returns the best overall performance. \n",
    "\n",
    "In the cell below, complete the `find_best_k()` function.  This function should:\n",
    "\n",
    "* take in six parameters:\n",
    "    * `X_train`, `y_train`, `X_test`, and  `y_test`\n",
    "    * `min_k` and `max_k`. Set these to `1` and `25`, by default\n",
    "* Create two variables, `best_k` and `best_score`\n",
    "* Iterate through every **_odd number_** between `min_k` and `max_k + 1`. \n",
    "* For each iteration:\n",
    "    * Create a new KNN classifier, and set the `n_neighbors` parameter to the current value for k, as determined by our loop.\n",
    "    * Fit this classifier to the training data.\n",
    "    * Generate predictions for `X_test` using the fitted classifier.\n",
    "    * Calculate the **_F1-score_** for these predictions.\n",
    "    * Compare this F1-score to `best_score`. If better, update `best_score` and `best_k`.\n",
    "* Once it has checked every value for `k`, print out the best value for k and the F1-score it achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_score = 0\n",
    "    best_k = 0\n",
    "    stats = []\n",
    "    for k in range(min_k, max_k):\n",
    "        if k == 1 or k % 2 != 0:\n",
    "            knn = KNeighborsClassifier(n_neighbors = k)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_hat_test = knn.predict(X_test)\n",
    "            score = f1_score(y_test, y_hat_test)\n",
    "            track = dict(k = k, f1 = score)\n",
    "            stats.append(track)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_k = k\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa2f18f32b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_stats = find_best_k(X_train, y_train, X_test, y_test)\n",
    "# Expected Output:\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Best Value for k: 3\n",
    "# F1-Score: 0.6444444444444444\n",
    "x_val = [item['k'] for item in k_stats]\n",
    "y_val = [item['f1'] for item in k_stats]\n",
    "best_k = 5\n",
    "best_f1 = 0.7612\n",
    "plt.plot(x_val, y_val, label = \"max f1_score = {}\".format(best_f1))\n",
    "plt.scatter(best_k, best_f1, c = \"red\", label = \"best k-value = {}\".format(best_k))\n",
    "plt.xlabel(\"k-values\")\n",
    "plt.ylabel(\"f1_score\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 5, 'f1': 0.7612903225806451}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(k_stats, key = lambda x: x['f1'])[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved our model performance by over 4 percent just by finding an optimal value for k. Good job! There are other parameters in the model that you can also tune. In a later section, we'll cover how we can automate the parameter search process using a technique called **_Grid Search_**. For, try playing around with the different options for parameters, and seeing how it affects model performance. For a full list of model parameters, see the [sklearn documentation !](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "## (Optional) Level Up: Iterating on the Data\n",
    "\n",
    "As an optional (but recommended!) exercise, think about the decisions we made during the preprocessing steps that could have affected our overall model performance. For instance, we replaced missing age values with the column median. Could this have affected ourn overall performance? How might the model have fared if we had just dropped those rows, instead of using the column median? What if we reduced dimensionality by ignoring some less important columns altogether?\n",
    "\n",
    "In the cells below, revisit your preprocessing stage and see if you can improve the overall results of the classifier by doing things differently. Perhaps you should consider dropping certain columns, or dealing with null values differently, or even using a different sort of scaling (or none at all!). Try a few different iterations on the preprocessing and see how it affects the overall performance of the model. The `find_best_k` function handles all of the fitting--use this to iterate quickly as you try different strategies for dealing with data preprocessing! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((714,), (714, 5))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_op = df.drop(columns = ['PassengerId', 'Name', 'Ticket','Cabin', 'SibSp', 'Parch', 'Embarked'])\n",
    "X_op.dropna(inplace = True)\n",
    "y = X_op.Survived\n",
    "y.shape, X_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Pclass', 'Age', 'Fare', 'Sex_female'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_op = pd.get_dummies(X_op)\n",
    "X_op.head()\n",
    "X_op.drop(columns = ['Sex_male'], inplace = True)\n",
    "X_op.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.7101449275362319\n",
      "Recall Score: 0.5444444444444444\n",
      "Accuracy Score: 0.659217877094972\n",
      "F1 Score: 0.6163522012578616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6163522012578616"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "features_poly = pd.DataFrame(poly.fit_transform(X_op), columns=poly.get_feature_names(X_op.columns))\n",
    "X_oppo= features_poly\n",
    "X_oppo.head()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_oppo, y, test_size=0.25, random_state = 18)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_poly = KNeighborsClassifier()\n",
    "knn_poly.fit(X_train, y_train)\n",
    "\n",
    "y_hat_test = knn_poly.predict(X_test)\n",
    "\n",
    "    \n",
    "print_metrics(y_test, y_hat_test)\n",
    "f1_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'k': 1, 'f1': 0.6410256410256411},\n",
       " {'k': 3, 'f1': 0.6193548387096774},\n",
       " {'k': 5, 'f1': 0.6163522012578616},\n",
       " {'k': 7, 'f1': 0.5974025974025974},\n",
       " {'k': 9, 'f1': 0.6114649681528662},\n",
       " {'k': 11, 'f1': 0.6013071895424835},\n",
       " {'k': 13, 'f1': 0.588235294117647},\n",
       " {'k': 15, 'f1': 0.620253164556962},\n",
       " {'k': 17, 'f1': 0.5960264900662251},\n",
       " {'k': 19, 'f1': 0.631578947368421},\n",
       " {'k': 21, 'f1': 0.608108108108108},\n",
       " {'k': 23, 'f1': 0.5906040268456376}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.7605633802816901\n",
      "Recall Score: 0.6\n",
      "Accuracy Score: 0.7039106145251397\n",
      "F1 Score: 0.670807453416149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.670807453416149"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_op, y, test_size=0.25, random_state = 18)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_poly = KNeighborsClassifier()\n",
    "knn_poly.fit(X_train, y_train)\n",
    "\n",
    "y_hat_test = knn_poly.predict(X_test)\n",
    "\n",
    "    \n",
    "print_metrics(y_test, y_hat_test)\n",
    "f1_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Good job! This concludes today's section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
